{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. 빅데이터 모델링\n",
    "\n",
    "## 02. 분석기법 적용\n",
    "---\n",
    "### 1.1 분석기법\n",
    "\n",
    "|-|KeyWord|\n",
    "|:--:|--|\n",
    "|회귀분석|회귀분석, 선형성, 독립성, 등분산성, 비상관성, 정상성, 최소제곱법, 회귀계수, 결정계수, F-통계량|\n",
    "|로지스틱<br>회귀분석|로지스틱회귀분석, 다중공선성|\n",
    "|의사결정나무|의사결정나무, 분류함수(분류규칙), 분리기준, 성장, 가지치기, 교차타당성, 카이제곱 통계량,<br>지니 지수, 엔트로피 지수, 순수도, CART, C4.5 & C5.0, CHAID, QUEST|\n",
    "|인공신경망|인공신경망, 퍼셉트론, 활성함수, XOR문제, 다층퍼셉트론, 역전파알고리즘, 기울기소실,<br>활성화함수, 계단함수, 부호함수, 시그모이드, tanh함수, ReLU, Leaky ReLU, Softmax 함수|\n",
    "|서포트<br>벡터 머신|SVM, 서포트벡터머신, 서포트벡터, 초평면, 슬랙변수, 커널트릭|\n",
    "|연관성 분석|연관성분석, 지지도, 신뢰도, 향상도|\n",
    "\n",
    "============================================================\n",
    "#### 1) 회귀분석\n",
    "##### (1) 회귀 분석(Regression Analysis)\n",
    " - 1개 이상의 독립변수가 종속변수에 미치는 영향을 추정\n",
    " - 변수들 사이의 인과관계를 밝히고 모형을 적합하여 관심있는 변수를 예측/추론\n",
    " - 변수: 수식에 따라서 변하는 값\n",
    "   - 영향을 주는 변수(x)/영향을 받는 변수(y)\n",
    "   - 영향을 주는 변수 = 독립변수 = 설명변수 = 예측변수\n",
    "   - 영향을 받는 변수 = 종속변수 = 반응변수 = 결과변수\n",
    " - 가정: 선형성 / 독립성 / 등분산성 / 비상관성 / 정상성\n",
    "   - 단순모형: 선형성 검증 / 다중모형: 5개 가정 모두 검증\n",
    "     - 선형성: 독립변수와 종속변수의 선형관계\n",
    "     - 독립성: 잔차와 독립변수 상관X\n",
    "     - 등분산성: 오차들의 분산 일정\n",
    "     - 비상관성: 오차들 간 상관X\n",
    "     - 정상성: 오차항(잔차항)이 정규분포\n",
    " - 모형 검증 체크리스트\n",
    "   - 통계적 유의미 / 회귀계수 / 설명력 / 데이터 적합 / 가정 만족\n",
    "   - 통계적 유의미: F-통계량, p-value 확인\n",
    "   - 회귀계수: 계수의 T-통계량, p-value, 신뢰구간 확인\n",
    "     - 계수(Coefficient): '인자'의 뜻으로 쓰이며 식 앞에 곱해지는 상수를 의미\n",
    "   - 설명력: 결정계수 확인\n",
    "   - 데이터 적합: 잔차 그래프 -> 회귀 진단\n",
    "   - 가정 만족: 5개 가정 모두 만족하는지\n",
    " - 편차 vs. 오차 vs. 잔차\n",
    "   - 편차(Deviation): 평균과의 차이 = 관측값이 평균값에서 떨어져 있는 정도\n",
    "   - 오차(Error): 모집단에서 실젯값과 회귀선의 차이 즉, 정확치와 관측값의 차이\n",
    "     - 예측하기 위한 추정치와 실젯값의 차이 = 예측값이 정확하지 못한 정도\n",
    "   - 잔차(Residual): 표본에서 나온 관측값과 회귀선의 차이\n",
    "     - 평균이 아닌, 회귀식 등으로 추정된 추정치와의 차이\n",
    "     - 추정된 값을 설명할 수 없어서 아직도 남아있는 편차 = 편차 일부분\n",
    "\n",
    "##### (2) 회귀 분석 유형\n",
    " - 단순선형 / 다중선형\n",
    " - 단순선형 회귀 분석(Simple Linear Regression Analysis)\n",
    "   - 독립변수 1개 / 종속변수 1개 / 오차항 있는 선형관계\n",
    "   - 회귀식: yi = β₀ + β₁xi + ei\n",
    "     - 오차항 ei는 독립적, N(0, σ²)의 분포\n",
    "   - 회귀계수 추정: 최소제곱법 사용하여 추정\n",
    "     - 최소제곱법(Least Square Method): 오차 제곱의 합이 가장 최소가 되는 회귀계수를 찾음\n",
    "   - 회귀분석 검정: 결정계수를 계산하여 결과가 적합한지 검증\n",
    "     - 회귀계수 검정: β₀ = 0 이면, 추정식은 의미없음\n",
    "     - 회귀직선 적합도/정확도 평가: 결정계수(R²) (0 ≤ R² ≤ 1) \n",
    "   - 선형회귀의 문제점\n",
    "     - 0 이하의 값 or 1 이상의 값을 예측값으로 줄 수 있음 -> 확률값으로 직접 해석할 수 없음\n",
    "   - 선형회귀와 제곱합\n",
    "   <img src=\"./Data/선형회귀예시.png\">  \n",
    "   <br>\n",
    "\n",
    " - 다중선형 회귀 분석(Multi Linear Regression Analysis)\n",
    "   - 독립변수 여러 개/ 종속변수 1개\n",
    "   - 모형의 통계적 유의성: F-통계량으로 확인\n",
    "     - F-통계량↑ p-value↓ -> p-value < 0.05 이면 귀무가설 기각 -> 모형이 통계적으로 유의\n",
    "     - F = MSR/MSE = (SSR/k) / {SSE/(n-k-1)}\n",
    "     - F-통계량: 분산이 동일하다고 가정되는 두 모집단으로부터, 독립적인 두 표본을 추출했을 때, 두 표본분산의 비율\n",
    "   - 회귀분석 검정\n",
    "     - 회귀계수: t-통계량\n",
    "     - 회귀선: 결정계수\n",
    "     - 모형적합성: 잔차와 종속변수의 산점도\n",
    "     - 다중공선성: VIF, 상태지수\n",
    "   - 다중공선성(Multicolinearity)\n",
    "     - 다중회귀분석에서 독립변수들 간 선형관계가 존재한다면 정확한 회귀계수 추정 어려움\n",
    "       - 분산팽창요인(VIF): 4 < VIF 다중공선성 존재 / 10 < VIF 심각한 문제\n",
    "       - 상태지수: 10 < 상태지수 이면 문제있음 / 30 < 상태지수 이면 심각\n",
    "       - 다중공선성 문제 발생 -> 변수 제거/주성분 회귀/능형 회귀 적용\n",
    "         - 주성분회귀(PCR): 독립변수들의 주성분들을 추출하여 회귀모델을 만드는 기법\n",
    "         - 능형회귀(Ridge Regression): 최소제곱합에 패널티 항을 추가하여 추정하여, 분산을 줄여주는 효과  \n",
    "         <br>\n",
    " - 주성분 분석: 서로 상관성이 높은 변수들을 선형결합으로 요약, 축소하는 기법\n",
    "   - 변수들의 분산 방식의 패턴을 간결하게 표현하는 주성분 변수를 원래 변수의 선형결합으로 추출하는 통계기법\n",
    "   - 분석을 통해 나타나는 주성분으로 변수들 사이의 구조를 쉽게 이해하는 건 어려움  \n",
    "   -> 요약하는 게 주 목적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================\n",
    "#### 2) 로지스틱 회귀분석\n",
    "##### (1) 로지스틱 회귀 분석(Logistic Regression Analysis)\n",
    " - 반응변수(종속변수)가 범주형, 분류 목적으로 사용\n",
    " - 새로운 설명변수(독립변수) 값이 주어질 때 반응변수(종속변수)의 각 범주에 속할 확률이 어느정도인지 추정하여 추정 확률을 기준치에 따라 분류\n",
    " - 클래스가 알려진 데이터에서 각 클래스내의 관측치들에 대한 유사성을 찾는 데 사용\n",
    " - 승산(오즈; Odds) = 실패에 비해 성공할 확률의 비 =  p / (1-p)\n",
    "   - 회귀식\n",
    "     - log( π(x) / (1-π(x)) ) = α + β₁x\n",
    "     - π(x) = P(Y=1 | x)\n",
    "   - 회귀계수 β₁ 부호에 따라 로지스틱 함수 그래프 모양이 달라짐  \n",
    "   -> β₁ > 0 - S자  \n",
    "   -> β₁ < 0 - 역 S자\n",
    "   - R 함수\n",
    "     - glm(): 모형 적합 함수\n",
    "     - cdplot(): 연속형변수의 변화에 따른 범주형변수의 조건부분포 조회 (탐색적 분석)\n",
    "     - step(): 변수 선택 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================\n",
    "#### 3) 의사결정나무\n",
    "##### (1) 의사결정나무(Decision Tree)\n",
    " - 분류함수를 활용하여, 의사결정규칙으로 이루어진 나무 모양을 그리는 기법\n",
    " - 데이터가 가진 속성들로부터 분할기준 속성을 판별하고 이에 따라 트리형태로 모델링하는 분류예측모델\n",
    " - 분류함수: 분류 기준으로 사용되는 함수\n",
    "   - 새로운 표본이 관측되었을 때 이 표본을 여러 모집단 중 어떤 하나의 모집단으로 분류하기 위한 함수\n",
    " - 시각화: 연속적인 의사결정문제 시각화 -> 의사결정 이루어지는 시점/성과파악을 쉽게 해줌\n",
    " - 해석용이: 계산결과가 직접적으로 나타남\n",
    "\n",
    "##### (2) 의사결정나무의 구성요소\n",
    " - 부모마디 / 자식마디 / 뿌리마디 / 끝마디 / 중간마디 / 가지 / 깊이  \n",
    " <img src=\"./Data/의사결정나무.png\">  \n",
    "\n",
    "##### (3) 해석력과 예측력\n",
    " - 해석력: 예를 들어, 은행에서 신용평가 결과 부적격판정인 경우, 이유를 해석할 수 있어야 함\n",
    " - 예측력: 예를 들어, 반응이 좋을 고객 모집방안을 알고자 하는 경우, 예측력에 집중해야 함\n",
    "\n",
    "##### (4) 의사결정나무의 분석\n",
    " - 분석 과정: 성장 -> 가지치기 -> 타당성평가 -> 해석및예측\n",
    "   - 성장(Growing): 분리규칙으로 나무성장 -> 정지규칙 만족 시 중단\n",
    "   - 가지치기(Pruning): 가지 제거(오류 위험/부적절한 추론규칙/불필요)\n",
    "   - 타당성 평가: 교차 타당성 등으로 평가(이익 도표/위험 도표/시험 자료 등을 이용)\n",
    "   - 해석 및 예측: 모형 해석 -> 데이터 분류 및 예측에 활용\n",
    " - 각 마디에서의 최적 분리규칙: 분리 변수 선택 & 분리 기준에 의해 결정됨\n",
    " - 분리변수의 P차원 공간에 대한 현재 분할은 이전 분할에 영향 받음\n",
    " - 성장(Growing): x 들로 이루어진 입력공간을 재귀적으로 분할하는 과정\n",
    "   - 분류 규칙(Splitting Rule): 최적 분할은 불순도 감소량을 가장 크게 하는 분할\n",
    "     - 연속형 분리변수: A = xi <= s\n",
    "     - 범주형 분리변수: A = 1,2,4/ Ac = 3\n",
    "   - 분리 기준(Splitting Criterion)\n",
    "     - 한 부모마디에서 자식마디들이 형성될 때, 입력변수의 선택과 범주의 병합이 이루어질 기준\n",
    "     - 순수도: 목표변수의 특정 범주에 개체들이 포함되어 있는 정도\n",
    "     - 순수도/불순도 측정 -> 목표변수의 분포를 가장 잘 구별해주는 자식마디 형성\n",
    "     - 부모보다 자식마디에서 순수도 증가\n",
    "   - 이산형 목표변수에 사용되는 분리기준\n",
    "     - 카이제곱 통계량의 p-value↓ / 지니 지수↓ / 엔트로피 지수 ↓\n",
    "     - p-value가 가장 작은 예측변수&분리\n",
    "     - 지니 지수를 가장 감소시켜주는 예측변수&분리\n",
    "     - 엔트로피 지수가 가장 작은 예측변수&분리\n",
    "   - 연속형 목표변수에 사용되는 분리기준\n",
    "     - 분산분석의 F-통계량 / 분산의 감소량\n",
    "     - F-통계량↑ p-value↓ p-value가 가장 작은 예측변수&분리\n",
    "     - 분산의 감소량을 최대화하는 기준&분리\n",
    "   - 정지 규칙(Stopping Rule)\n",
    "     - 현재 마디가 끝마디가 되도록 하는 규칙\n",
    "     - 나무 깊이 지정 / 끝마디 레코드 최소 개수 지정\n",
    "       \n",
    "<img src=\"./Data/의사결정나무분석.png\">\n",
    "\n",
    " - 가지치기(Pruning)\n",
    "   - 과대/과소 적합을 방지하기 위해 의사결정나무의 가지를 제거함\n",
    "   - 의사결정나무의 크기 = 복잡도  \n",
    "   -> 크기가 너무 크면 과대적합 / 너무 작으면 과소적합 위험\n",
    "   - 최적의 크기(복잡도)는 대상자료로부터 추정\n",
    "   - 분류 오류를 크게할 위험 or 부적절한 규칙을 가진 가지를 제거함\n",
    "   - 나무의 끝마디가 너무 나오면, 모형이 과대적합되어 규칙을 현실 문제에 적용할 수 없음  \n",
    "   -> 분류된 관측치의 비율 or MSE 등을 고려하여 과적합 문제를 해결하기 위해 가지치기를 함\n",
    "\n",
    "##### (5) 의사결정나무 알고리즘\n",
    " - CART / C4.5 & C5.0 / CHAID / QUEST  \n",
    " <img src=\"./Data/의사결정나무알고리즘.png\">\n",
    "\n",
    " - 편향(Bias): 학습 알고리즘에서 잘못된 가정을 했을 때 발생하는 오차\n",
    "\n",
    "##### (6) 의사결정나무 종류\n",
    " - 분류나무 / 회귀나무 모형\n",
    " - 의사결정나무는 주어진 입력값에 대해 출력값을 예측하는 모형\n",
    "\n",
    "##### (7) 의사결정나무 활용 및 장단점\n",
    " - 활용: 세분화 / 분류 / 예측 / 차원축소 및 변수선택 / 교호작용 효과 파악\n",
    "   - 차원축소 및 변수선택: 목표변수에 큰 영향을 미치는 예측변수들을 구분하고자 할 때\n",
    "   - 교호작용 효과 파악: 여러 예측변수 결합  \n",
    "   -> 범주의 병합 or 연속형 변수의 이산화\n",
    "     - 교호작용(Interaction): 독립변수간 상호작용이 종속변수에 영향을 주는 현상\n",
    " - 장점: 해석 용이 / 상호작용 효과 해석 가능 / 비모수적 모형 / 유연성 및 정확도 높음\n",
    "   - 비모수적 모형: 가정 필요X, 이상값에 민감X\n",
    "   - 유연성 및 정확도 높음: 대용량 데이터에서도 빠르게 생성 가능\n",
    " - 단점: 비연속성 / 선형성 or 주효과 결여 / 비안정성\n",
    "\n",
    "   - 비연속성: 연속형변수를 비연속적 값으로 취급 -> 경계점 근방에서 예측오류 가능성 큼\n",
    "   - 선형성 or 주효과 결여: 선형모형에서는 각 변수의 영향력을 해석할 수 있는데, 의사결정나무는 불가능\n",
    "   - 비안정성: Training Data에만 의존하면 과대적합 가능성 -> 검증용데이터로 교차타당성 평가 or 가지치기 필요\n",
    " - 평가: 이익 도표 or 검정용 데이터에 의한 교차 타당성 등을 이용하여, 의사결정나무를 평가함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================\n",
    "#### 4) 인공신경망\n",
    "##### (1) 인공신경망\n",
    " - 뉴런의 전기신호 전달을 모방한 기계학습 모델\n",
    " - 인공신경망(ANN; Artificial Neural Network)\n",
    "   - 입력값 받아서 출력값 만들기 위해 활성화 함수 사용함\n",
    " - 활성화 함수/활성 함수(Activation Function)\n",
    "   - 입력신호의 총합을 출력신호로 변환하는 함수\n",
    "   - 입력받은 신호를 얼마나 출력할지 결정\n",
    "   - 출력된 신호의 활성화 여부 결정\n",
    " - 신경망 모형의 특징\n",
    "   - 변수가 많은 경우나 입출력 변수간 복잡한 비선형 관계일 때 유용함\n",
    "   - 잡음에 민감하지 않음\n",
    "   - 은닉층 너무 많으면, 과대적합 위험\n",
    "   - 은닉층 너무 적으면, 충분한 데이터 표현X\n",
    "\n",
    "##### (2) 인공신경망의 역사\n",
    " - 퍼셉트론과 XOR 선형 분리 불가 문제 -> 다층 퍼셉트론과 기울기 소실 문제 -> 인공지능과 딥러닝  \n",
    " <img src=\"./Data/인공신경망역사.png\">\n",
    "\n",
    "##### (3) 인공신경망의 구조\n",
    " - 퍼셉트론 / 다층 퍼셉트론\n",
    " - 퍼셉트론(Perceptron) 구성\n",
    "   - 입력값 / 가중치 / 순 입력함수 / 활성함수/ 출력값(예측값)\n",
    "   - 입력값: 훈련 데이터(Training Data)\n",
    "   - 순 입력함수: 함수에서 모든 입력값과 가중치를 곱하고 Sum\n",
    "   - 활성 함수\n",
    "     - 순 입력함수에서 나온 값과 임계값 비교 -> 출력값(예측값)으로 1 or -1\n",
    "     - 예측값 != 실젯값 -> 가중치 업데이트 -> 이 과정을 반복하면서 학습\n",
    " - 퍼셉트론 문제점: XOR 선형 분리 불가 문제 -> 해결 위해 다층 퍼셉트론 등장\n",
    "   - AND 연산: 입력값 (X, Y) 이 모두 1이면 1 출력 / 나머지는 0 → 선형분리 가능\n",
    "   - OR 연산: 입력값 (X, Y) 이 모두 0이면 0 출력/ 나머지는 1 → 선형분리 가능\n",
    "   - XOR 연산: 입력값 (X, Y) 이 같으면 0 출력/ 다르면 1 출력 → 선형분리 불가능\n",
    " - 퍼셉트론의 구조  \n",
    " <img src=\"./Data/퍼셉트론구조.png\">\n",
    "\n",
    " - 다층 퍼셉트론(MLP; Multi-Layer Perceptrons)\n",
    "   - 비선형적으로 분리되는 데이터에 대한 학습이 가능한 퍼셉트론\n",
    "   - 구성: 입력층과 출력층 사이에 1개 이상의 은닉층\n",
    "   - 활성화 함수: 시그모이드 함수(Sigmoid Function)\n",
    "     - 시그모이드: 유한한 영역 가짐/미분가능/모든 점에서 음이 아닌 미분값/하나의 변곡점\n",
    "   - 역전파 알고리즘을 통해 다층에서 학습 가능\n",
    "     - 예측값과 실젯값의 차이인 에러(Error)를 통해 가중치 조정 -> 연결 강도 갱신 -> 목적함수 최적화\n",
    " - 다층 퍼셉트론의 문제점: 과대 적합 / 기울기 소실\n",
    "   - 과대 적합: 학습 데이터가 부족하면 실제 데이터에서 성능 떨어짐 -> 빅데이터 확보 가능해지면서 해결\n",
    "   - 기울기 소실: 시그모이드 함수의 편미분을 진행하면 기울기가 0에 근사 -> ReLU, tanh 함수 사용하여 해결\n",
    " - 다층 퍼셉트론의 구조  \n",
    " <img src=\"./Data/다층퍼셉트론구조.png\">\n",
    "\n",
    "##### (4) 뉴런의 활성화 함수\n",
    " - 순 입력함수에서 전달받은 값을 출력값으로 변환하는 함수\n",
    " - 계단 / 부호 / 시그모이드 / tanh / ReLU / Leaky ReLU / Softmax 함수\n",
    " - Dying ReLU: ReLU 함수에서 마이너스(-) 값 -> 전부 0을 출력 -> 일부 가중치들이 업데이트 되지 않음\n",
    "   \n",
    "<img src=\"./Data/활성화함수.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================\n",
    "#### 5) 서포트 벡터 머신\n",
    "##### (1) 서포트 벡터 머신(SVM; Support Vector Machine)\n",
    " - 지도학습 / 이진선형분류\n",
    " - 서포트 벡터 머신\n",
    "   - 데이터들과의 거리가 가장 먼 초평면을 선택하여 분리하는 지도학습 기반의 이진 선형 분류 모델\n",
    " - 기준: 초평면(Hyperplane)을 기준으로 데이터를 분리함\n",
    " - 활용: 사물 / 패턴 / 손글씨 숫자 인식 등\n",
    " - 서포트 벡터 머신 특징\n",
    "   - 공간상 최적의 분리 초평면을 찾음 -> 분류 및 회귀\n",
    "   - 변수 속성 간 의존성 고려X\n",
    "   - 모든 속성 활용\n",
    "   - 훈련시간 느린 편 / 그러나 정확성↑ \n",
    "   - 다른 방법보다 과대적합 가능성↓\n",
    "   - R package: e1071, kernlab, klaR 등  \n",
    "   <br>\n",
    " - 서포트 벡터 머신  \n",
    " <img src=\"./Data/서포트벡터머신.png\">\n",
    "\n",
    "##### (2) 서포트 벡터 머신 종류\n",
    " - 하드 마진 SVM / 소프트 마진 SVM\n",
    " - 하드 마진(Hard Margin): 오분류 허용X -> 노이즈로 최적의 결정경계 잘못 찾음 or 못 찾음\n",
    " - 소프트 마진(Soft Margin): 오분류 허용O -> 어느정도 오류를 허용하는 소프트 마진을 주로 이용함\n",
    "\n",
    "##### (3) 서포트 벡터 머신의 구성요소\n",
    " - 결정경계 / 초평면 / 마진 / 서포트벡터 / 슬랙변수(여유변수)\n",
    "\n",
    "결정 경계 (Decision Boundary): 데이터 분류 기준\n",
    "초평면 (Hyperplane): N차원 공간의 (N-1)차원 평면 (데이터 분리)\n",
    "마진 (Margin, 여유공간): 결정 경계 ~ 서포트 벡터 간 거리 → 이 마진을 최대화하는 것이 최적의 결정 경계\n",
    "서포트 벡터 (Support Vector): 결정 경계와 가장 가까운 데이터들의 집합 (학습 데이터 중에서)\n",
    "슬랙 변수 (Slack Variable, 여유변수): 완벽한 분리 불가능할 경우 → 허용된 오차를 위한 변수 (소프트 마진 SVM에서)\n",
    "\n",
    " \n",
    " \n",
    "(4) 서포트 벡터 머신 적용 기준 | 선형으로 분리 가능/ 불가능\n",
    "\n",
    "선형 분리 가능 SVM: 최적 결정 경계(초평면) 기준으로 +1 과 -1 로 구분 → 분류 모델\n",
    "선형 분리 불가능 SVM: 커널 트릭 활용\n",
    "\n",
    "커널 함수: 저차원에서 함수의 계산만으로 원하는 풀이가 가능한 함수\n",
    "커널 트릭: 커널 함수를 이용하여, 고차원 공간으로 매핑하면서 증가하는 연산량의 문제를 해결하는 기법\n",
    "따라서, 저차원 공간을 고차원 공간으로 매핑할 때 발생하는 연산의 복잡성을 커널 트릭으로 해결할 수 있다~!\n",
    "예를 들어, 2차원에서 분류할 수 없는 문제를 → 3차원 공간에 매핑하여 선형 분류한다.\n",
    "대표적인 커널 함수: 가우시안 RBF 커널/ 다항식 커널/ 시그모이드 커널 등- 커널 함수 선택에 명확한 규칙 X  정확도 차이 별로 X\n",
    "출처: https://sy-log.tistory.com/26?category=992358 [서윤로그:티스토리]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. 빅데이터 모델링\n",
    "\n",
    "## 02. 분석기법 적용\n",
    "---\n",
    "### 1.1 분석기법\n",
    "\n",
    "|KeyWord|회귀분석, 선형성, 독립성, 등분산성, 비상관성, 정상성, 최소제곱법, 회귀계수, 결정계수, F-통계량, 로지스틱회귀분석, 다중공선성|\n",
    "|:--:|--|\n",
    "\n",
    "============================================================\n",
    "#### 1) 회귀분석\n",
    "##### (1) 회귀 분석(Regression Analysis)\n",
    "\n",
    "<img src=\"./Data/빅데이터_플랫폼.png\">\n",
    "<img src=\"./Data/빅데이터_플랫폼.png\"  width=\"400\" height=\"400\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
