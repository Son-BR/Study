{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. 빅데이터 모델링\n",
    "\n",
    "## 02. 분석기법 적용\n",
    "---\n",
    "### 2.1 분석기법\n",
    "\n",
    "|-|KeyWord|\n",
    "|:--:|--|\n",
    "|회귀분석|회귀분석, 선형성, 독립성, 등분산성, 비상관성, 정상성, 최소제곱법, 회귀계수, 결정계수, F-통계량|\n",
    "|로지스틱<br>회귀분석|로지스틱회귀분석, 다중공선성|\n",
    "|의사결정나무|의사결정나무, 분류함수(분류규칙), 분리기준, 성장, 가지치기, 교차타당성, 카이제곱 통계량,<br>지니 지수, 엔트로피 지수, 순수도, CART, C4.5 & C5.0, CHAID, QUEST|\n",
    "|인공신경망|인공신경망, 퍼셉트론, 활성함수, XOR문제, 다층퍼셉트론, 역전파알고리즘, 기울기소실,<br>활성화함수, 계단함수, 부호함수, 시그모이드, tanh함수, ReLU, Leaky ReLU, Softmax 함수|\n",
    "|서포트<br>벡터 머신|SVM, 서포트벡터머신, 서포트벡터, 초평면, 슬랙변수, 커널트릭|\n",
    "|연관성 분석|연관성분석, 지지도, 신뢰도, 향상도|\n",
    "|군집 분석|군집분석, 계층적군집, k-평균군집, 혼합평균군집, EM알고리즘, 자기조직화지도(SOM), 최단연결법, 최장연결법, 중심연결법, 평균연결법, 와드연결법, 유클리드, 맨하튼, 민코프스키, 표준화, 마할라노비스 거리, 단순일치계수, 자카드계수, 순위상관계수|\n",
    "\n",
    "============================================================\n",
    "#### 1) 회귀분석\n",
    "##### (1) 회귀 분석(Regression Analysis)\n",
    " - 1개 이상의 독립변수가 종속변수에 미치는 영향을 추정\n",
    " - 변수들 사이의 인과관계를 밝히고 모형을 적합하여 관심있는 변수를 예측/추론\n",
    " - 변수: 수식에 따라서 변하는 값\n",
    "   - 영향을 주는 변수(x)/영향을 받는 변수(y)\n",
    "   - 영향을 주는 변수 = 독립변수 = 설명변수 = 예측변수\n",
    "   - 영향을 받는 변수 = 종속변수 = 반응변수 = 결과변수\n",
    " - 가정: 선형성 / 독립성 / 등분산성 / 비상관성 / 정상성\n",
    "   - 단순모형: 선형성 검증 / 다중모형: 5개 가정 모두 검증\n",
    "     - 선형성: 독립변수와 종속변수의 선형관계\n",
    "     - 독립성: 잔차와 독립변수 상관X\n",
    "     - 등분산성: 오차들의 분산 일정\n",
    "     - 비상관성: 오차들 간 상관X\n",
    "     - 정상성: 오차항(잔차항)이 정규분포\n",
    " - 모형 검증 체크리스트\n",
    "   - 통계적 유의미 / 회귀계수 / 설명력 / 데이터 적합 / 가정 만족\n",
    "   - 통계적 유의미: F-통계량, p-value 확인\n",
    "   - 회귀계수: 계수의 T-통계량, p-value, 신뢰구간 확인\n",
    "     - 계수(Coefficient): '인자'의 뜻으로 쓰이며 식 앞에 곱해지는 상수를 의미\n",
    "   - 설명력: 결정계수 확인\n",
    "   - 데이터 적합: 잔차 그래프 -> 회귀 진단\n",
    "   - 가정 만족: 5개 가정 모두 만족하는지\n",
    " - 편차 vs. 오차 vs. 잔차\n",
    "   - 편차(Deviation): 평균과의 차이 = 관측값이 평균값에서 떨어져 있는 정도\n",
    "   - 오차(Error): 모집단에서 실젯값과 회귀선의 차이 즉, 정확치와 관측값의 차이\n",
    "     - 예측하기 위한 추정치와 실젯값의 차이 = 예측값이 정확하지 못한 정도\n",
    "   - 잔차(Residual): 표본에서 나온 관측값과 회귀선의 차이\n",
    "     - 평균이 아닌, 회귀식 등으로 추정된 추정치와의 차이\n",
    "     - 추정된 값을 설명할 수 없어서 아직도 남아있는 편차 = 편차 일부분\n",
    "\n",
    "##### (2) 회귀 분석 유형\n",
    " - 단순선형 / 다중선형\n",
    " - 단순선형 회귀 분석(Simple Linear Regression Analysis)\n",
    "   - 독립변수 1개 / 종속변수 1개 / 오차항 있는 선형관계\n",
    "   - 회귀식: yi = β₀ + β₁xi + ei\n",
    "     - 오차항 ei는 독립적, N(0, σ²)의 분포\n",
    "   - 회귀계수 추정: 최소제곱법 사용하여 추정\n",
    "     - 최소제곱법(Least Square Method): 오차 제곱의 합이 가장 최소가 되는 회귀계수를 찾음\n",
    "   - 회귀분석 검정: 결정계수를 계산하여 결과가 적합한지 검증\n",
    "     - 회귀계수 검정: β₀ = 0 이면, 추정식은 의미없음\n",
    "     - 회귀직선 적합도/정확도 평가: 결정계수(R²) (0 ≤ R² ≤ 1) \n",
    "   - 선형회귀의 문제점\n",
    "     - 0 이하의 값 or 1 이상의 값을 예측값으로 줄 수 있음 -> 확률값으로 직접 해석할 수 없음\n",
    "   - 선형회귀와 제곱합\n",
    "   <img src=\"./Data/선형회귀예시.png\">  \n",
    "   <br>\n",
    "\n",
    " - 다중선형 회귀 분석(Multi Linear Regression Analysis)\n",
    "   - 독립변수 여러 개/ 종속변수 1개\n",
    "   - 모형의 통계적 유의성: F-통계량으로 확인\n",
    "     - F-통계량↑ p-value↓ -> p-value < 0.05 이면 귀무가설 기각 -> 모형이 통계적으로 유의\n",
    "     - F = MSR/MSE = (SSR/k) / {SSE/(n-k-1)}\n",
    "     - F-통계량: 분산이 동일하다고 가정되는 두 모집단으로부터, 독립적인 두 표본을 추출했을 때, 두 표본분산의 비율\n",
    "   - 회귀분석 검정\n",
    "     - 회귀계수: t-통계량\n",
    "     - 회귀선: 결정계수\n",
    "     - 모형적합성: 잔차와 종속변수의 산점도\n",
    "     - 다중공선성: VIF, 상태지수\n",
    "   - 다중공선성(Multicolinearity)\n",
    "     - 다중회귀분석에서 독립변수들 간 선형관계가 존재한다면 정확한 회귀계수 추정 어려움\n",
    "       - 분산팽창요인(VIF): 4 < VIF 다중공선성 존재 / 10 < VIF 심각한 문제\n",
    "       - 상태지수: 10 < 상태지수 이면 문제있음 / 30 < 상태지수 이면 심각\n",
    "       - 다중공선성 문제 발생 -> 변수 제거/주성분 회귀/능형 회귀 적용\n",
    "         - 주성분회귀(PCR): 독립변수들의 주성분들을 추출하여 회귀모델을 만드는 기법\n",
    "         - 능형회귀(Ridge Regression): 최소제곱합에 패널티 항을 추가하여 추정하여, 분산을 줄여주는 효과  \n",
    "         <br>\n",
    " - 주성분 분석: 서로 상관성이 높은 변수들을 선형결합으로 요약, 축소하는 기법\n",
    "   - 변수들의 분산 방식의 패턴을 간결하게 표현하는 주성분 변수를 원래 변수의 선형결합으로 추출하는 통계기법\n",
    "   - 분석을 통해 나타나는 주성분으로 변수들 사이의 구조를 쉽게 이해하는 건 어려움  \n",
    "   -> 요약하는 게 주 목적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================\n",
    "#### 2) 로지스틱 회귀분석\n",
    "##### (1) 로지스틱 회귀 분석(Logistic Regression Analysis)\n",
    " - 반응변수(종속변수)가 범주형, 분류 목적으로 사용\n",
    " - 새로운 설명변수(독립변수) 값이 주어질 때 반응변수(종속변수)의 각 범주에 속할 확률이 어느정도인지 추정하여 추정 확률을 기준치에 따라 분류\n",
    " - 클래스가 알려진 데이터에서 각 클래스내의 관측치들에 대한 유사성을 찾는 데 사용\n",
    " - 승산(오즈; Odds) = 실패에 비해 성공할 확률의 비 =  p / (1-p)\n",
    "   - 회귀식\n",
    "     - log( π(x) / (1-π(x)) ) = α + β₁x\n",
    "     - π(x) = P(Y=1 | x)\n",
    "   - 회귀계수 β₁ 부호에 따라 로지스틱 함수 그래프 모양이 달라짐  \n",
    "   -> β₁ > 0 - S자  \n",
    "   -> β₁ < 0 - 역 S자\n",
    "   - R 함수\n",
    "     - glm(): 모형 적합 함수\n",
    "     - cdplot(): 연속형변수의 변화에 따른 범주형변수의 조건부분포 조회 (탐색적 분석)\n",
    "     - step(): 변수 선택 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================\n",
    "#### 3) 의사결정나무\n",
    "##### (1) 의사결정나무(Decision Tree)\n",
    " - 분류함수를 활용하여, 의사결정규칙으로 이루어진 나무 모양을 그리는 기법\n",
    " - 데이터가 가진 속성들로부터 분할기준 속성을 판별하고 이에 따라 트리형태로 모델링하는 분류예측모델\n",
    " - 분류함수: 분류 기준으로 사용되는 함수\n",
    "   - 새로운 표본이 관측되었을 때 이 표본을 여러 모집단 중 어떤 하나의 모집단으로 분류하기 위한 함수\n",
    " - 시각화: 연속적인 의사결정문제 시각화 -> 의사결정 이루어지는 시점/성과파악을 쉽게 해줌\n",
    " - 해석용이: 계산결과가 직접적으로 나타남\n",
    "\n",
    "##### (2) 의사결정나무의 구성요소\n",
    " - 부모마디 / 자식마디 / 뿌리마디 / 끝마디 / 중간마디 / 가지 / 깊이  \n",
    " <img src=\"./Data/의사결정나무.png\">  \n",
    "\n",
    "##### (3) 해석력과 예측력\n",
    " - 해석력: 예를 들어, 은행에서 신용평가 결과 부적격판정인 경우, 이유를 해석할 수 있어야 함\n",
    " - 예측력: 예를 들어, 반응이 좋을 고객 모집방안을 알고자 하는 경우, 예측력에 집중해야 함\n",
    "\n",
    "##### (4) 의사결정나무의 분석\n",
    " - 분석 과정: 성장 -> 가지치기 -> 타당성평가 -> 해석및예측\n",
    "   - 성장(Growing): 분리규칙으로 나무성장 -> 정지규칙 만족 시 중단\n",
    "   - 가지치기(Pruning): 가지 제거(오류 위험/부적절한 추론규칙/불필요)\n",
    "   - 타당성 평가: 교차 타당성 등으로 평가(이익 도표/위험 도표/시험 자료 등을 이용)\n",
    "   - 해석 및 예측: 모형 해석 -> 데이터 분류 및 예측에 활용\n",
    " - 각 마디에서의 최적 분리규칙: 분리 변수 선택 & 분리 기준에 의해 결정됨\n",
    " - 분리변수의 P차원 공간에 대한 현재 분할은 이전 분할에 영향 받음\n",
    " - 성장(Growing): x 들로 이루어진 입력공간을 재귀적으로 분할하는 과정\n",
    "   - 분류 규칙(Splitting Rule): 최적 분할은 불순도 감소량을 가장 크게 하는 분할\n",
    "     - 연속형 분리변수: A = xi <= s\n",
    "     - 범주형 분리변수: A = 1,2,4/ Ac = 3\n",
    "   - 분리 기준(Splitting Criterion)\n",
    "     - 한 부모마디에서 자식마디들이 형성될 때, 입력변수의 선택과 범주의 병합이 이루어질 기준\n",
    "     - 순수도: 목표변수의 특정 범주에 개체들이 포함되어 있는 정도\n",
    "     - 순수도/불순도 측정 -> 목표변수의 분포를 가장 잘 구별해주는 자식마디 형성\n",
    "     - 부모보다 자식마디에서 순수도 증가\n",
    "   - 이산형 목표변수에 사용되는 분리기준\n",
    "     - 카이제곱 통계량의 p-value↓ / 지니 지수↓ / 엔트로피 지수 ↓\n",
    "     - p-value가 가장 작은 예측변수&분리\n",
    "     - 지니 지수를 가장 감소시켜주는 예측변수&분리\n",
    "     - 엔트로피 지수가 가장 작은 예측변수&분리\n",
    "   - 연속형 목표변수에 사용되는 분리기준\n",
    "     - 분산분석의 F-통계량 / 분산의 감소량\n",
    "     - F-통계량↑ p-value↓ p-value가 가장 작은 예측변수&분리\n",
    "     - 분산의 감소량을 최대화하는 기준&분리\n",
    "   - 정지 규칙(Stopping Rule)\n",
    "     - 현재 마디가 끝마디가 되도록 하는 규칙\n",
    "     - 나무 깊이 지정 / 끝마디 레코드 최소 개수 지정\n",
    "       \n",
    "<img src=\"./Data/의사결정나무분석.png\">\n",
    "\n",
    " - 가지치기(Pruning)\n",
    "   - 과대/과소 적합을 방지하기 위해 의사결정나무의 가지를 제거함\n",
    "   - 의사결정나무의 크기 = 복잡도  \n",
    "   -> 크기가 너무 크면 과대적합 / 너무 작으면 과소적합 위험\n",
    "   - 최적의 크기(복잡도)는 대상자료로부터 추정\n",
    "   - 분류 오류를 크게할 위험 or 부적절한 규칙을 가진 가지를 제거함\n",
    "   - 나무의 끝마디가 너무 나오면, 모형이 과대적합되어 규칙을 현실 문제에 적용할 수 없음  \n",
    "   -> 분류된 관측치의 비율 or MSE 등을 고려하여 과적합 문제를 해결하기 위해 가지치기를 함\n",
    "\n",
    "##### (5) 의사결정나무 알고리즘\n",
    " - CART / C4.5 & C5.0 / CHAID / QUEST  \n",
    " <img src=\"./Data/의사결정나무알고리즘.png\">\n",
    "\n",
    " - 편향(Bias): 학습 알고리즘에서 잘못된 가정을 했을 때 발생하는 오차\n",
    "\n",
    "##### (6) 의사결정나무 종류\n",
    " - 분류나무 / 회귀나무 모형\n",
    " - 의사결정나무는 주어진 입력값에 대해 출력값을 예측하는 모형\n",
    "\n",
    "##### (7) 의사결정나무 활용 및 장단점\n",
    " - 활용: 세분화 / 분류 / 예측 / 차원축소 및 변수선택 / 교호작용 효과 파악\n",
    "   - 차원축소 및 변수선택: 목표변수에 큰 영향을 미치는 예측변수들을 구분하고자 할 때\n",
    "   - 교호작용 효과 파악: 여러 예측변수 결합  \n",
    "   -> 범주의 병합 or 연속형 변수의 이산화\n",
    "     - 교호작용(Interaction): 독립변수간 상호작용이 종속변수에 영향을 주는 현상\n",
    " - 장점: 해석 용이 / 상호작용 효과 해석 가능 / 비모수적 모형 / 유연성 및 정확도 높음\n",
    "   - 비모수적 모형: 가정 필요X, 이상값에 민감X\n",
    "   - 유연성 및 정확도 높음: 대용량 데이터에서도 빠르게 생성 가능\n",
    " - 단점: 비연속성 / 선형성 or 주효과 결여 / 비안정성\n",
    "\n",
    "   - 비연속성: 연속형변수를 비연속적 값으로 취급 -> 경계점 근방에서 예측오류 가능성 큼\n",
    "   - 선형성 or 주효과 결여: 선형모형에서는 각 변수의 영향력을 해석할 수 있는데, 의사결정나무는 불가능\n",
    "   - 비안정성: Training Data에만 의존하면 과대적합 가능성 -> 검증용데이터로 교차타당성 평가 or 가지치기 필요\n",
    " - 평가: 이익 도표 or 검정용 데이터에 의한 교차 타당성 등을 이용하여, 의사결정나무를 평가함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================\n",
    "#### 4) 인공신경망\n",
    "##### (1) 인공신경망\n",
    " - 뉴런의 전기신호 전달을 모방한 기계학습 모델\n",
    " - 인공신경망(ANN; Artificial Neural Network)\n",
    "   - 입력값 받아서 출력값 만들기 위해 활성화 함수 사용함\n",
    " - 활성화 함수/활성 함수(Activation Function)\n",
    "   - 입력신호의 총합을 출력신호로 변환하는 함수\n",
    "   - 입력받은 신호를 얼마나 출력할지 결정\n",
    "   - 출력된 신호의 활성화 여부 결정\n",
    " - 신경망 모형의 특징\n",
    "   - 변수가 많은 경우나 입출력 변수간 복잡한 비선형 관계일 때 유용함\n",
    "   - 잡음에 민감하지 않음\n",
    "   - 은닉층 너무 많으면, 과대적합 위험\n",
    "   - 은닉층 너무 적으면, 충분한 데이터 표현X\n",
    "\n",
    "##### (2) 인공신경망의 역사\n",
    " - 퍼셉트론과 XOR 선형 분리 불가 문제 -> 다층 퍼셉트론과 기울기 소실 문제 -> 인공지능과 딥러닝  \n",
    " <img src=\"./Data/인공신경망역사.png\">\n",
    "\n",
    "##### (3) 인공신경망의 구조\n",
    " - 퍼셉트론 / 다층 퍼셉트론\n",
    " - 퍼셉트론(Perceptron) 구성\n",
    "   - 입력값 / 가중치 / 순 입력함수 / 활성함수/ 출력값(예측값)\n",
    "   - 입력값: 훈련 데이터(Training Data)\n",
    "   - 순 입력함수: 함수에서 모든 입력값과 가중치를 곱하고 Sum\n",
    "   - 활성 함수\n",
    "     - 순 입력함수에서 나온 값과 임계값 비교 -> 출력값(예측값)으로 1 or -1\n",
    "     - 예측값 != 실젯값 -> 가중치 업데이트 -> 이 과정을 반복하면서 학습\n",
    " - 퍼셉트론 문제점: XOR 선형 분리 불가 문제 -> 해결 위해 다층 퍼셉트론 등장\n",
    "   - AND 연산: 입력값 (X, Y) 이 모두 1이면 1 출력 / 나머지는 0 → 선형분리 가능\n",
    "   - OR 연산: 입력값 (X, Y) 이 모두 0이면 0 출력/ 나머지는 1 → 선형분리 가능\n",
    "   - XOR 연산: 입력값 (X, Y) 이 같으면 0 출력/ 다르면 1 출력 → 선형분리 불가능\n",
    " - 퍼셉트론의 구조  \n",
    " <img src=\"./Data/퍼셉트론구조.png\">\n",
    "\n",
    " - 다층 퍼셉트론(MLP; Multi-Layer Perceptrons)\n",
    "   - 비선형적으로 분리되는 데이터에 대한 학습이 가능한 퍼셉트론\n",
    "   - 구성: 입력층과 출력층 사이에 1개 이상의 은닉층\n",
    "   - 활성화 함수: 시그모이드 함수(Sigmoid Function)\n",
    "     - 시그모이드: 유한한 영역 가짐/미분가능/모든 점에서 음이 아닌 미분값/하나의 변곡점\n",
    "   - 역전파 알고리즘을 통해 다층에서 학습 가능\n",
    "     - 예측값과 실젯값의 차이인 에러(Error)를 통해 가중치 조정 -> 연결 강도 갱신 -> 목적함수 최적화\n",
    " - 다층 퍼셉트론의 문제점: 과대 적합 / 기울기 소실\n",
    "   - 과대 적합: 학습 데이터가 부족하면 실제 데이터에서 성능 떨어짐 -> 빅데이터 확보 가능해지면서 해결\n",
    "   - 기울기 소실: 시그모이드 함수의 편미분을 진행하면 기울기가 0에 근사 -> ReLU, tanh 함수 사용하여 해결\n",
    " - 다층 퍼셉트론의 구조  \n",
    " <img src=\"./Data/다층퍼셉트론구조.png\">\n",
    "\n",
    "##### (4) 뉴런의 활성화 함수\n",
    " - 순 입력함수에서 전달받은 값을 출력값으로 변환하는 함수\n",
    " - 계단 / 부호 / 시그모이드 / tanh / ReLU / Leaky ReLU / Softmax 함수\n",
    " - Dying ReLU: ReLU 함수에서 마이너스(-) 값 -> 전부 0을 출력 -> 일부 가중치들이 업데이트 되지 않음\n",
    "   \n",
    "<img src=\"./Data/활성화함수.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================\n",
    "#### 5) 서포트 벡터 머신\n",
    "##### (1) 서포트 벡터 머신(SVM; Support Vector Machine)\n",
    " - 지도학습 / 이진선형분류\n",
    " - 서포트 벡터 머신\n",
    "   - 데이터들과의 거리가 가장 먼 초평면을 선택하여 분리하는 지도학습 기반의 이진 선형 분류 모델\n",
    " - 기준: 초평면(Hyperplane)을 기준으로 데이터를 분리함\n",
    " - 활용: 사물 / 패턴 / 손글씨 숫자 인식 등\n",
    " - 서포트 벡터 머신 특징\n",
    "   - 공간상 최적의 분리 초평면을 찾음 -> 분류 및 회귀\n",
    "   - 변수 속성 간 의존성 고려X\n",
    "   - 모든 속성 활용\n",
    "   - 훈련시간 느린 편 / 그러나 정확성↑ \n",
    "   - 다른 방법보다 과대적합 가능성↓\n",
    "   - R package: e1071, kernlab, klaR 등  \n",
    "   <br>\n",
    " - 서포트 벡터 머신  \n",
    " <img src=\"./Data/서포트벡터머신.png\">\n",
    "\n",
    "##### (2) 서포트 벡터 머신 종류\n",
    " - 하드 마진 SVM / 소프트 마진 SVM\n",
    " - 하드 마진(Hard Margin): 오분류 허용X -> 노이즈로 최적의 결정경계 잘못 찾음 or 못 찾음\n",
    " - 소프트 마진(Soft Margin): 오분류 허용O -> 어느정도 오류를 허용하는 소프트 마진을 주로 이용함\n",
    "\n",
    "##### (3) 서포트 벡터 머신의 구성요소\n",
    " - 결정경계 / 초평면 / 마진 / 서포트벡터 / 슬랙변수(여유변수)\n",
    " - 결정 경계(Decision Boundary): 데이터 분류 기준\n",
    " - 초평면(Hyperplane): N차원 공간의 (N-1)차원 평면(데이터 분리)\n",
    " - 마진(Margin, 여유공간): 결정 경계 ~ 서포트 벡터 간 거리 -> 이 마진을 최대화하는 것이 최적의 결정 경계\n",
    " - 서포트 벡터(Support Vector): 결정 경계와 가장 가까운 데이터들의 집합(학습 데이터 중에서)\n",
    " - 슬랙 변수(Slack Variable, 여유변수): 완벽한 분리 불가능할 경우 -> 허용된 오차를 위한 변수(소프트 마진 SVM에서)\n",
    "\n",
    "##### (4) 서포트 벡터 머신 적용 기준\n",
    " - 선형으로 분리 가능/불가능 여부\n",
    " - 선형 분리 가능 SVM: 최적 결정 경계(초평면) 기준으로 +1 과 -1 로 구분 -> 분류 모델\n",
    " - 선형 분리 불가능 SVM: 커널 트릭 활용\n",
    "   - 커널 함수: 저차원에서 함수의 계산만으로 원하는 풀이가 가능한 함수\n",
    "   - 커널 트릭: 커널 함수를 이용하여, 고차원 공간으로 매핑하면서 증가하는 연산량의 문제를 해결하는 기법  \n",
    "   -> 따라서, 저차원 공간을 고차원 공간으로 매핑할 때 발생하는 연산의 복잡성을 커널 트릭으로 해결가능\n",
    "   - (예) 2차원에서 분류할 수 없는 문제를 3차원 공간에 매핑하여 선형 분류\n",
    "   - 대표적인 커널 함수: 가우시안 RBF 커널/다항식 커널/시그모이드 커널 등\n",
    "     - 커널 함수 선택에 명확한 규칙X, 정확도 차이 별로X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================\n",
    "#### 6) 연관성 분석\n",
    "##### (1) 연관성 분석(Association Analysis)\n",
    " - 데이터 간 관계에서 조건과 반응을 연결하는 분석\n",
    " - 연관성 분석 = 장바구니 분석 = 서열 분석\n",
    "   - 데이터 내부에 존재하는 항목간 상호관계 or 종속관계를 찾아내는 분석방법\n",
    " - 연관성 분석 특징\n",
    "   - 목적변수X -> 분석 방향 or 목적 없어도 적용 가능\n",
    "   - 조건-반응(IF-THEN)으로 표현-> 결과 이해 쉬움\n",
    "   - 계산 매우 간단\n",
    "   - 세분화 특징\n",
    "     - 장점: 적절한 세분화로 인한 품목 결정\n",
    "     - 단점: 너무 세분화되면 의미 없는 결과\n",
    "\n",
    "##### (2) 연관성 분석의 주요 용어\n",
    " - 지지도 / 신뢰도 / 향상도  \n",
    " <img src=\"./Data/연관성분석.png\">\n",
    " \n",
    " - 향상도 (Lift): 1을 기준으로 A, B 사이의 상관관계 측정\n",
    "   - 향상도 = 1 : A, B가 서로 독립적\n",
    "   - 향상도 < 1 : A, B가 음의 상관관계(-)\n",
    "   - 향상도 > 1 : A, B가 양의 상관관계(+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================\n",
    "#### 7) 군집 분석\n",
    "##### (1) 군집 분석(Cluster Analysis)\n",
    " - 데이터를 집단화 / 다변량 분석기법\n",
    " - 여러 개의 변숫값들로부터 유사성(Similarity)만 기초로 n개의 군집으로 집단화하여 집단의 특성을 분석하는 다변량 분석기법\n",
    " - 군집 분석 종류: 계층적 군집/k-평균 군집/혼합 분포 군집/자기 조직화 지도(SOM)\n",
    " - 계층적 군집: 군집 개수 미리 정하지 않음 -> 병합적 방법/분할적 방법/덴드로그램\n",
    " - 비계층적 군집: 군집 개수 k 미리 정함 -> k-평균 군집/혼합 분포 군집/자기 조직화 지도\n",
    "\n",
    "##### (2) 계층적 군집(Hierarchical Clustering)\n",
    " - 유사한 개체들의 군집화 과정 반복\n",
    " - 군집 형성 방법: 병합적 방법 / 분할적 방법\n",
    "   - 병합적 방법(Agglomerative): 작은 군집 -> 병합\n",
    "     - 거리 가까우면 유사성 높음\n",
    "     - R : {stats} hclust(), {cluster} agnes(), mclust()\n",
    "   - 분할적 방법(Divisive): 큰 군집 -> 분리\n",
    "     - R : {cluster} diana(), mona()\n",
    " - 군집 결과 표현: 계통도 / 덴드로그램\n",
    "   - 덴드로그램(Dendrogram): 군집의 개체들이 결합되는 순서를 나타내는 트리 구조\n",
    "   - 항목간 거리/군집간 거리/군집내 항목간 유사도/군집의 견고성 파악 가능\n",
    "   - 각 개체는 한 군집에만 속함\n",
    " - 군집간 거리 측정 방법/연결법: 최단연결법/최장연결법/중심연결법/평균연결법/와드연결법\n",
    "   - 최단연결법 = 단일연결법: 두 군집간 거리 = 최솟값 으로 측정\n",
    "     - 각 군집에서 한 개체씩 뽑았을 때 나타날 수 있는 최솟값을 군집간 거리로 측정함\n",
    "   - 최장연결법 = 완전연결법: 두 군집간 거리 = 최댓값 으로 측정\n",
    "     - 각 군집에서 한 개체씩 뽑았을 때 나타날 수 있는 최댓값을 군집간 거리로 측정함\n",
    "   - 중심연결법: 두 군집 중심 사이 거리 측정\n",
    "     - 두 군집 결합 -> 가중평균으로 새로운 군집의 평균 구함\n",
    "     - 군집 내 편차 제곱합 고려 -> 군집간 정보 손실을 최소화\n",
    "   - 평균연결법: 모든 개체에 대한 거리 평균 구하면서 군집화\n",
    "     - 계산량이 불필요하게 많아질 가능성 존재\n",
    "   - 와드연결법: 군집내 오차 제곱합 기반으로 군집화\n",
    "     - 다른 연결법들은 군집간 거리에 기반하는데, 와드연결법은 군집내 거리를 기반으로 함\n",
    " - 군집간 거리 계산: 연속형/명목형/순서형 변수마다 거리 계산 방법 다름\n",
    "   - 연속형 변수 거리: 유클리드/맨하튼/민코프스키/표준화/마할라노비스 거리\n",
    "     - 유클리드 거리: 두 점을 잇는 가장 짧은 직선 거리\n",
    "     - 맨하튼 거리(시가 거리): 각 방향 직각의 이동 거리 합\n",
    "     - 민코프스키 거리: 1차원일 때 맨하튼 거리, 2차원일 때 유클리드 거리와 같음\n",
    "     - 표준화 거리: 각 변수를 표준편차로 변환한 후, 유클리드 거리를 계산\n",
    "     - 마할라노비스 거리: 변수들의 산포를 고려하여 표준화한 거리  \n",
    "     변수의 표준편차 고려 / 변수간 상관성이 있으면 표준화 거리 사용 검토해야 함\n",
    "   - 명목형 변수 거리: 단순 일치 계수 / 자카드 계수\n",
    "     - 모든 변수가 명목형인 경우, (두 개체간 다른 값을 가지는 변수의 수)를 (총 변수의 수)로 나눈 것이 거리임\n",
    "     - 자카드 계수는 두 집합이 같으면 1, 공통 원소가 없으면 0\n",
    "   - 순서형 변수 거리: 순위상관계수  \n",
    "   \n",
    " <img src=\"./Data/변수거리.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) k-평균 군집(k-means clustering)\n",
    " - k 개의 군집 묶음 -> 군집 평균 재계산 -> 반복\n",
    " - 주어진 데이터를 k개의 군집으로 묶는 알고리즘\n",
    "   - 초기 군집을 k개 지정하고 각 개체를 가까운 군집에 할당하여 군집을 형성한 다음 각 군집 평균을 재계산하고 군집 갱신을 반복하여 k개의 최종 군집을 형성\n",
    " - 절차: k개 객체 선택 -> 할당 -> 중심 갱신 -> 반복\n",
    "   - k개 객체 선택: 초기 군집의 중심으로 삼을 객체 k개를 랜덤 선택\n",
    "   - 할당(Assignment): 각 객체들을 가장 가까운 군집의 중심에 할당\n",
    "   - 중심 갱신(New Centroids): 각 군집별로 평균 계산 -> 군집 중심 갱신\n",
    "   - 반복: 군집 중심의 변화가 거의 없을 때까지/최대 반복수에 도달할 때까지 할당과 중심 갱신을 반복\n",
    " - 단점: 이상값에 민감 -> k-중앙값 군집 or 이상값 미리 제거\n",
    "\n",
    "##### (4) 혼합 분포 군집(Mixture Distribution Clustering)\n",
    " - 모수적 모형 기반 군집화 방법\n",
    " - 혼합 분포 군집\n",
    "   - 데이터가 k개의 모수적 모형의 가중합으로 표현되는 모집단 모형에서 나왔다는 가정 하에 데이터로부터 모수&가중치를 추정하는 방법\n",
    " - k개의 모형 = k개의 군집 을 의미함\n",
    " - 군집화 방법\n",
    "   - 추정된 k개의 모형(군집)들 중에서 어느 모형에서 나왔을 확률이 높은지에 따라서 각각의 데이터를 군집으로 분류\n",
    " - 혼합모형 = M개 분포(성분)의 가중합\n",
    "   - 단일모형과 비교하면, 혼합모형은 표현식이 복잡함 -> 미분을 통한 이론적 전개가 어려움\n",
    "   - 최대가능도 추정을 위해 EM알고리즘을 활용함\n",
    " - EM 알고리즘(Expectation-Maximization Algorithm, 기댓값 최대화 알고리즘)\n",
    "   - 관찰/측정되지 않은 잠재변수에 의존하는 확률모델에서 최대 가능도나 최대 사후 확률을 가지는 모수의 추정값을 찾는 반복적인 알고리즘\n",
    "     - 최대 가능도(Maximum Likelihood): 어떤 모수가 주어졌을 때, 원하는 값들이 나올 가능도를 최대로 만드는 모수를 선택하는 방법\n",
    " - 진행과정: E-step -> M-step\n",
    "   - E-step: 잠재변수 Z의 기대치 계산\n",
    "   - M-step: 기대치 이용하여 파라미터 추정\n",
    "   - 반복: M-step에서 계산된 값은 다음 E-step에서 추정값으로 쓰임\n",
    " - 특징\n",
    "   - 확률분포 도입하여 군집화\n",
    "   - 군집을 모수로 표현\n",
    "   - 서로 다른 크기의 군집 찾을 수 있음\n",
    "   - 데이터 커지면 수렴 시간 걸림\n",
    "   - 군집 크기 너무 작으면 추정 정도 떨어짐\n",
    "   - 이상값 민감 -> 사전 조치 필요\n",
    "\n",
    "##### (5) 자기 조직화 지도(SOM; Self-Organizing Maps): 비지도 신경망 클러스터링\n",
    " - 자기 조직화 지도\n",
    "   - 대뇌피질, 시각피질의 학습과정을 기반으로 모델화한 인공신경망\n",
    "   - 자율학습방법에 의한 클러스터링 방법을 적용한 알고리즘\n",
    "   - 고차원 데이터를 이해하기 쉬운 저차원 뉴런으로 정렬 -> 지도로 형상화한 비지도 신경망\n",
    " - 구성: 입력층 / 경쟁층  \n",
    " <img src=\"./Data/자기조직화지도.png\">  \n",
    "\n",
    " - 특징: 입력변수의 위치관계를 그대로 보존하여 형상화 -> 실제 공간에 가까이 있으면 지도상에도 가까운 위치\n",
    " - 학습과정: 경쟁학습 / 승자독식구조\n",
    "   - 경쟁학습: 경쟁층의 각 뉴런이 입력벡터와 얼마나 가까운지 계산  \n",
    "   -> 연결강도를 반복적으로 재조정하여 학습  \n",
    "   -> 입력패턴과 가장 유사한 경쟁층 뉴런이 승자\n",
    "   - 승자독식구조: 경쟁층에 승자 뉴런만 나타남  \n",
    "   -> 승자와 유사한 연결강도를 가지는 입력패턴이 동일한 경쟁 뉴런으로 배열됨\n",
    "   - 연결강도 초기화 -> 입력벡터 제시 -> 유사도 계산 -> 프로토타입 벡터 탐색 -> 연결강도 재조정 -> 반복\n",
    "     - 프로토타입 벡터 탐색: 입력벡터와 가장 가까운 뉴런인 BMU(Best Matching Unit)을 탐색하는 단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.2 고급 분석기법\n",
    "\n",
    "|-|KeyWord|\n",
    "|:--:|--|\n",
    "|범주형<br>자료 분석|분할표분석, 상대위험도, 승산비, 카이제곱분석, 교차분석, 적합도검정, 독립성검정, 동질성검정, 피셔정확검정|\n",
    "|다변량 분석|다변량분석, 피어슨상관계수, 스피어만상관계수, 다차원척도법, 주성분분석|\n",
    "|시계열 분석|시계열분석, 정상성, 자기회귀모형(AR모형), 이동평균모형(MA모형), 자기회귀 누적 이동평균모형(ARIMA모형),<br>백색잡음과정, 분해시계열, 추세요인, 계절요인, 순환요인, 불규칙 요인|\n",
    "|베이지안<br>기법|확률, 교사건, 표본 평균, 표본 분산, 표본 표준편차, 표본 공분산, 상관계수, 상관계수 행렬, 조건부확률, 전확률의정리, 베이즈정리, 베이즈확률|\n",
    "\n",
    "============================================================\n",
    "#### 1) 범주형 자료 분석\n",
    "##### (1) 범주형 자료 분석\n",
    " - 분할표 분석 / 카이제곱 분석 / 피셔 정확 검정 / 로지스틱 회귀분석\n",
    " - 범주형 자료 분석\n",
    "   - 종속변수: 1개 / 범주형\n",
    "   - 종속변수가 1개이고 범주형인 데이터를 분석하여, 모형과 독립변수의 유의성을 알아봄\n",
    " - 독립변수(X)의 척도에 따른 분석방법\n",
    "   - 독립변수가 범주형: 분할표 분석/카이제곱 검정(교차 분석)/피셔 정확 검정\n",
    "   - 독립변수가 연속형: 로지스틱 회귀분석\n",
    "\n",
    "##### (2) 분할표 분석(Contingency Table)\n",
    " - 상대위험도(RR) / 승산비(Odds Ratio)  \n",
    " <img src=\"./Data/이원분할표.png\">\n",
    "\n",
    " - 분할표\n",
    "   - 범주형 변수 개수에 따라 1원/2원/다원 분할표\n",
    "   - 행: 독립변수 / 열: 종속변수\n",
    "   - 주변합(Margin Sum): 마지막 행열에 총계 데이터\n",
    " - 상대위험도(RR; Relative Risk) = {a/(a+b)}/{c/(c+d)}: 비교 집단의 위험률 대비 관심있는 집단의 위험률\n",
    "   - (관심 집단 위험률)/(비교 집단 위험률)= {a/(a+b)}/{c/(c+d)}\n",
    "   - 위험률: 특정 사건이 발생할 비율\n",
    "   - 1을 기준으로 평가함\n",
    "     - RR < 1: 관심 집단의 특정 사건 발생확률 낮음\n",
    "     - RR = 1: 관심 집단과 특정 사건의 발생에 연관성 없음\n",
    "     - RR > 1: 관심 집단의 특정 사건 발생확률 높음\n",
    " - 승산비(Odds Ratio) = 교차비 = 대응위험도\n",
    "   - 승산 = p / (1 - p): 특정 사건이 발생하지 않을 확률 대비, 발생할 확률  \n",
    "    = (특정 사건 발생 확률) / (발생하지 않을 확률)  \n",
    "    = (이길 확률) / (1 - 이길확률) = p / (1 - p)\n",
    "   - 승산비 = ad / bc: 비교 집단의 승산 대비, 관심 있는 집단의 승산  \n",
    "   = (관심 집단의 승산) / (비교 집단의 승산)  \n",
    "   = (특정 조건이 있을 때의 오즈) / (다른 조건이 있을 때의 오즈)  \n",
    "   = (a / b) / (c / d) = ad / bc (교차비)\n",
    "\n",
    "##### (3) 카이제곱 분석(Chi-Squared Test) = 교차 분석\n",
    " - 적합도 검정 / 독립성 검정 / 동질성 검정\n",
    " - 분석 방법: χ²(카이제곱 값) 계산 -> p-value가 유의수준보다 작으면 귀무가설 기각\n",
    "   - 카이제곱 값=Σ(관측빈도-기대빈도)²/(기대빈도)\n",
    "     - 관측빈도와 기대빈도의 차이인 편차의 제곱 값을 기대빈도로 나눈 값들의 합\n",
    "  \n",
    "<img src=\"./Data/검정구분.png\">\n",
    "\n",
    "##### (4) 피셔의 정확 검정(Fisher's Exact Test)\n",
    " - 분할표 문제로 카이제곱검정의 정확도가 떨어지는 경우 사용하는 방법\n",
    " - 피셔 정확 검정을 사용하는 경우\n",
    "   - 표본 수가 적은 경우\n",
    "   - 분할표에서 셀에 치우치게 분포된 경우\n",
    "   - 기대빈도가 5 미만인 셀이 20% 넘는 경우\n",
    "     - 기대빈도: 두 변수가 독립일 경우에 이론적으로 기대할 수 있는 빈도 분포 / 예상되는 빈도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================\n",
    "#### 2) 다변량 분석\n",
    "##### (1) 상관분석\n",
    " - 피어슨 상관계수 / 스피어만 순위 상관계수\n",
    " - 피어슨 상관계수: 두 변수간 선형관계의 크기를 측정(비선형관계는 측정X)\n",
    "   - 등간 척도 or 비례 척도를 사용하는 연속형 데이터에서 사용\n",
    "   - 계산방법: 두 변수의 공분산을 표준편차의 곱으로 나눈 값  \n",
    "   = Corr (X, Y) = Cov (X, Y) / √Var(X)Var(Y)\n",
    "   - 모집단 - 모 상관계수 (ρ) / 표본집단 - 표본 상관계수 (r)\n",
    "   - -1 에서 +1 사이의 값\n",
    " - 스피어만 순위 상관계수: 두 변수간 비선형적인 관계도 나타낼 수 있음\n",
    "   - 계산방법: 두 변수를 모두 순위로 변환 -> 두 순위간 피어슨 상관계수 계산\n",
    "   - -1 에서 +1 사이의 값\n",
    "\n",
    "##### (2) 다차원 척도법\n",
    " - 개체들 사이의 유사성(비유사성)을 측정하여 시각적으로 표현\n",
    " - 다차원 척도법(MDS; MultiDimensionality Scaling)\n",
    "   - 개체들 사이의 유사성, 비유사성을 측정하여 2차원 or 3차원 공간상에 점으로 표현하여 개체들 사이의 집단화를 시각적으로 표현하는 분석 방법\n",
    " - 여러 대상간의 거리가 주어져 있을 때, 대상들을 동일한 상대적 거리를 가진 실수 공간의 점들로 배치시키는 방법\n",
    " - 주어진 거리는 추상적인 대상들 간 거리 & 실수 공간에서의 거리 둘 다 될 수 있음\n",
    " - 주로 자료들의 상대적 관계를 이해하는 시각화 방법의 근간으로 사용됨  \n",
    "\n",
    "<img src=\"./Data/다차원척도법.png\">\n",
    "\n",
    "##### (3) 주성분 분석\n",
    " - 고차원 자료의 차원을 축소(축약)시킴 / 상관성 높은 변수들을 요약함\n",
    " - 주성분 분석(PCA; Principal Component Analysis)\n",
    "   - 상관관계가 있는 고차원 자료의 원래 변동을 최대한 보존하여 저차원 자료로 변환하는 차원축소 방법\n",
    " - 차원축소: 많은 변수들로 구성된 고차원 자료를 축소하여, 새로운 차원의 자료를 생성하는 기법\n",
    "   - 고윳값(Eigenvalue)이 높은 순서로 정렬 -> 높은 고윳값을 가진 고유벡터(Eigenvector)만으로 데이터 복원\n",
    " - 주성분 분석의 특징\n",
    "   - p개의 변수들을 중요한 m(P)개의 주성분으로 표현 -> 전체 변동을 설명\n",
    "     - p차원 변수 X = (X1, X2, ... , Xp)^T\n",
    "   - 주성분은 원래 변수들의 선형결합으로 표현됨\n",
    "     - 기존의 상관성이 높은 변수들을 요약, 축소시킴\n",
    "   - 차원 감소폭 결정: 스크린 산점도/전체 변이의 공헌도/평균 고윳값 등을 활용\n",
    "   - 누적 기여율 85% 이상 -> 주성분 수로 결정 가능\n",
    "     - 누적 기여율: 제 1 ~ k 주성분까지의 주성분을 이용하여 설명할 수 있는 데이터 전체 정보량의 비율\n",
    "   - 주성분으로 변수들 사이의 구조를 쉽게 이해하기는 어려움\n",
    "   - 차원의 저주 해결\n",
    "     - 데이터 차원이 증가할 때, 데이터 구조를 변환하여 정보를 최대한 축적하는 차원 감소방법으로 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================\n",
    "#### 3) 시계열 분석\n",
    "##### (1) 시계열 분석\n",
    "  - 연도별/분기별/월별 등 시계열로 관측되는 자료를 분석 -> 미래 예측\n",
    " - 시계열로 관측 -> x축 시간 & y축 관측값\n",
    " - 추세를 분석 -> 미래를 예측\n",
    " - 시계열 데이터: 규칙적 / 불규칙적인 특징 가짐\n",
    "\n",
    "##### (2) 정상성(Stationary)\n",
    " - 시점에 상관없이 시계열의 특성이 일정함\n",
    " - 정상성을 만족해야, 시계열 분석이 가능함\n",
    " - 정상성 조건: 평균 일정/분산&공분산이 시점에 의존하지 않음/공분산은 시차에만 의존함\n",
    " - 기댓값과 분포가 시점에 따라서 달라지지 않는다면, 정상성을 만족한다고 할 수 있음\n",
    "\n",
    "##### (3) 시계열 모형\n",
    " - 자기회귀모형(AR모형)/이동평균모형(MA모형)/자기회귀 누적 이동평균모형(ARIMA모형)\n",
    " - 자기 회귀 모형(AR모형)(Auto-Regressive Model)\n",
    "   - 현시점의 자료가 p시점 전의 유한 개의 과거 자료로 설명될 수 있음\n",
    "   - 과거 p번째까지의 데이터가 현재 데이터에 영향을 준다면 AR(p) 모형\n",
    "   - 관심있는 변수의 과거 값들의 선형결합을 이용하여, 자기 자신에 대한 미래 값을 예측\n",
    "   - 과거 관측값의 오차항이 미래 관측값에 영향을 줌\n",
    " - 이동평균 모형(MA모형)(Moving Average Model)\n",
    "   - 시간이 지날수록 관측치의 평균값이 지속적으로 증가or감소하는 시계열 모형\n",
    "   - 현시점의 자료가 p시점 전의 유한 개의 과거 백색잡음의 선형결합으로 표현됨\n",
    "   - 자신의 과거 값을 사용하여 설명하는 시계열 모형 -> 정상 확률 모형/항상 정상성 만족 -> 정상성가정 필요없음\n",
    "   -  과거의 연속적인 오차항이 현재/미래 관측값에 영향을 줌\n",
    "   - [백색잡음(오차항)의 현재값] & [자기자신의 과거값]의 선형 가중합\n",
    " - 자기 회귀 누적 이동평균 모형(ARIMA모형)(Auto Regressive Integrated Moving Average Model)\n",
    "   - 분기/반기/연간 단위로 다음 지표를 예측하거나, 주간/월간 단위로 지표를 리뷰하여 트렌드를 분석\n",
    "   - 시계열의 비정상성을 설명하기 위해, 관측치간의 차분을 사용\n",
    "   - 비정상 시계열 모형 -> 차분 or 변환 -> AR or MA or ARMA 모형으로 정상화\n",
    "   - Integrated는 누적을 의미하는데, 차분을 이용하는 시계열 모형에서 이 표현을 사용\n",
    " - 백색잡음(White Noise): 모든 개별 확률변수들이 서로 독립이고 동일한 확률분포를 따르는 확률과정을 말함(I.I.D.)\n",
    " -백색잡음과정(White Noise Process): 백색잡음과정 a(t)는 독립이고 같은 분포를 따르며 확률변수임(평균 = 0)  \n",
    "\n",
    "<img src=\"./Data/시계열모형.png\">\n",
    "\n",
    " - 분해 시계열: 시계열에 영향 주는 일반적인 요인을 시계열에서 분리하여 분석하는 방법 -> 분해식 사용\n",
    "   - 회귀 분석적인 방법을 주로 사용  \n",
    "   -> 관찰된 연속형 변수들에 대해, 두 변수 사이의 모형을 구한 뒤 적합도를 측정하는 분석 방법\n",
    "   - 시계열 구성요소: 추세 / 계절 / 순환 / 불규칙 요인  \n",
    "\n",
    "\n",
    "<img src=\"./Data/분해시계열.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================\n",
    "#### 4) 베이지안 기법\n",
    "##### (1) 확률 및 기본 통계이론\n",
    " - 확률 (Probability)\n",
    "   - 비슷한 현상이 반복해서 일어날 경우 어떤 사건이 발생할 가능성을 숫자로 표현하는 방법\n",
    "   - 같은 원인에서 특정한 결과가 나타나는 비율\n",
    " - 교사건(Intersection of Events)\n",
    "   - P(A⋂B) = 사건 A, B에 동시에 속하는 기본 결과들의 모임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sy-log.tistory.com/30?category=992358"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. 빅데이터 모델링\n",
    "\n",
    "## 02. 분석기법 적용\n",
    "---\n",
    "### 1.1 분석기법\n",
    "\n",
    "|-|KeyWord|\n",
    "|:--:|--|\n",
    "|||\n",
    "\n",
    "============================================================\n",
    "#### 1) 회귀분석\n",
    "##### (1) 회귀 분석(Regression Analysis)\n",
    "\n",
    "<img src=\"./Data/빅데이터_플랫폼.png\">\n",
    "<img src=\"./Data/빅데이터_플랫폼.png\"  width=\"400\" height=\"400\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
